//! API request/response models for model deployments.

pub mod enrichment;

use super::pagination::Pagination;
use crate::api::models::groups::GroupResponse;
use crate::db::models::deployments::{
    DeploymentDBResponse, ModelType, ProviderPricing, ProviderPricingUpdate, TokenPricing, TokenPricingUpdate,
};
use crate::types::{DeploymentId, InferenceEndpointId, UserId};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use serde_with::rust::double_option;
use utoipa::{IntoParams, ToSchema};
use uuid::Uuid;

/// Query parameters for listing deployed models
#[derive(Debug, Deserialize, IntoParams, ToSchema)]
pub struct ListModelsQuery {
    /// Pagination parameters
    #[serde(flatten)]
    #[param(inline)]
    pub pagination: Pagination,
    /// Filter by inference endpoint ID
    #[param(value_type = Option<String>, format = "uuid")]
    #[schema(value_type = Option<String>, format = "uuid")]
    pub endpoint: Option<InferenceEndpointId>,
    /// Include related data (comma-separated: "groups", "metrics", "status", "pricing", "endpoints")
    pub include: Option<String>,
    /// Show deleted models when true, non-deleted when false, all when not specified (admin only for deleted=true)
    pub deleted: Option<bool>,
    /// Show inactive models when true, active when false, all when not specified (admin only for inactive=true)
    pub inactive: Option<bool>,
    /// Filter to only models the current user can access (defaults to false for admins, true for users)
    pub accessible: Option<bool>,
    /// Search query to filter models by alias or model_name (case-insensitive substring match)
    pub search: Option<String>,
}

/// Query parameters for getting a single deployed model
#[derive(Debug, Deserialize, IntoParams, ToSchema)]
pub struct GetModelQuery {
    /// Show deleted model when true, 404 when false/unspecified if model is deleted
    pub deleted: Option<bool>,
    /// Show inactive model when true, 404 when false/unspecified if model is inactive
    pub inactive: Option<bool>,
    /// Include related data (comma-separated: "groups", "metrics", "status", "pricing", "endpoints")
    pub include: Option<String>,
}

/// Time series point for model activity sparklines
#[derive(Debug, Clone, Serialize, Deserialize, ToSchema)]
pub struct ModelTimeSeriesPoint {
    pub timestamp: DateTime<Utc>,
    pub requests: i64,
}

/// Model metrics for display on model cards
#[derive(Debug, Clone, Serialize, Deserialize, ToSchema)]
pub struct ModelMetrics {
    /// Average latency across all requests in milliseconds
    pub avg_latency_ms: Option<f64>,
    /// Total number of requests made to this model
    pub total_requests: i64,
    /// Total input tokens processed by this model
    pub total_input_tokens: i64,
    /// Total output tokens generated by this model
    pub total_output_tokens: i64,
    /// When the model was last active (last request timestamp)
    pub last_active_at: Option<DateTime<Utc>>,
    /// Recent activity for sparklines (last 24 hours, hourly buckets)
    pub time_series: Option<Vec<ModelTimeSeriesPoint>>,
}

/// The data required to create a new model.
#[derive(Debug, Clone, Serialize, Deserialize, ToSchema)]
pub struct DeployedModelCreate {
    /// The actual model identifier (e.g., "gpt-4", "claude-3-sonnet")
    pub model_name: String,
    /// User-friendly alias (e.g., "GPT-4 Turbo", "Claude Sonnet") - defaults to model_name if not provided
    pub alias: Option<String>,
    /// Inference endpoint ID where the model is hosted
    #[schema(value_type = String, format = "uuid")]
    pub hosted_on: InferenceEndpointId,
    /// Optional description of the model
    pub description: Option<String>,
    /// Optional model type (Chat or Embeddings)
    pub model_type: Option<ModelType>,
    /// Optional array of model capabilities
    pub capabilities: Option<Vec<String>>,
    /// Global per-model rate limit: requests per second (null = no limit)
    pub requests_per_second: Option<f32>,
    /// Global per-model rate limit: maximum burst size (null = no limit)
    pub burst_size: Option<i32>,
    /// Maximum number of concurrent requests allowed for this model (null = no limit)
    pub capacity: Option<i32>,
    /// Maximum number of concurrent batch requests allowed for this model (null = defaults to capacity or no limit)
    pub batch_capacity: Option<i32>,
    /// Customer-facing pricing rates
    pub pricing: Option<TokenPricing>,
    /// Provider/downstream pricing details (admin only)
    pub downstream_pricing: Option<ProviderPricing>,
}

/// The data required to update a specific model.
#[derive(Debug, Clone, Serialize, Deserialize, ToSchema)]
pub struct DeployedModelUpdate {
    pub alias: Option<String>,
    pub description: Option<Option<String>>,
    pub model_type: Option<Option<ModelType>>,
    pub capabilities: Option<Option<Vec<String>>>,
    /// Global per-model rate limit: requests per second (null = no change, Some(None) = remove limit)
    #[serde(default, skip_serializing_if = "Option::is_none", with = "double_option")]
    pub requests_per_second: Option<Option<f32>>,
    /// Global per-model rate limit: maximum burst size (null = no change, Some(None) = remove limit)
    #[serde(default, skip_serializing_if = "Option::is_none", with = "double_option")]
    pub burst_size: Option<Option<i32>>,
    /// Maximum concurrent requests (null = no change, Some(None) = remove limit, Some(Some(n)) = set limit)
    #[serde(default, skip_serializing_if = "Option::is_none", with = "double_option")]
    pub capacity: Option<Option<i32>>,
    /// Maximum concurrent batch requests (null = no change, Some(None) = remove limit, Some(Some(n)) = set limit)
    #[serde(default, skip_serializing_if = "Option::is_none", with = "double_option")]
    pub batch_capacity: Option<Option<i32>>,
    /// Customer-facing pricing rates partial updates (null = no change, Some(pricing_update) = partial update)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub pricing: Option<TokenPricingUpdate>,
    /// Provider/downstream pricing details partial updates (null = no change, Some(pricing_update) = partial update)
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub downstream_pricing: Option<ProviderPricingUpdate>,
}

/// A request to update a specific model (i.e. bundle a `DeployedModelUpdate` with a model id).
#[derive(Debug, Clone, Serialize, Deserialize, ToSchema)]
pub struct DeployedModelUpdateRequest {
    #[schema(value_type = String, format = "uuid")]
    pub id: DeploymentId,
    pub deployed_model: DeployedModelUpdate,
}

/// Probe status information for a model (only included when include=status)
#[derive(Debug, Clone, Serialize, Deserialize, ToSchema)]
pub struct ModelProbeStatus {
    #[schema(value_type = Option<String>, format = "uuid")]
    pub probe_id: Option<Uuid>,
    pub active: bool,
    pub interval_seconds: Option<i32>,
    pub last_check: Option<DateTime<Utc>>,
    pub last_success: Option<bool>,
    pub uptime_percentage: Option<f64>,
}

/// API response for a deployed model
#[derive(Debug, Clone, Serialize, Deserialize, ToSchema)]
pub struct DeployedModelResponse {
    #[schema(value_type = String, format = "uuid")]
    pub id: DeploymentId,
    pub model_name: String,
    pub alias: String,
    pub description: Option<String>,
    pub model_type: Option<ModelType>,
    pub capabilities: Option<Vec<String>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[schema(value_type = Option<String>, format = "uuid")]
    pub created_by: Option<UserId>,
    #[schema(value_type = String, format = "uuid")]
    pub hosted_on: InferenceEndpointId,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    /// Global per-model rate limit: requests per second (null = no limit)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub requests_per_second: Option<f32>,
    /// Global per-model rate limit: maximum burst size (null = no limit)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub burst_size: Option<i32>,
    /// Maximum number of concurrent requests allowed for this model (null = no limit)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub capacity: Option<i32>,
    /// Maximum number of concurrent batch requests allowed for this model (null = defaults to capacity or no limit)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub batch_capacity: Option<i32>,
    /// Groups that have access to this model (only included if requested)
    /// Note: no_recursion is important! utoipa will panic at runtime, because it overflows the
    /// stack trying to follow the relationship.
    #[serde(skip_serializing_if = "Option::is_none")]
    #[schema(no_recursion)]
    pub groups: Option<Vec<GroupResponse>>,
    /// Model usage metrics (only included if requested)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub metrics: Option<ModelMetrics>,
    /// Probe status (only included if requested)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub status: Option<ModelProbeStatus>,
    /// Customer-facing pricing rates (only included if requested)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub pricing: Option<TokenPricing>,
    /// Provider/downstream pricing details (only included if requested and user has Pricing::ReadAll)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub downstream_pricing: Option<ProviderPricing>,
    /// Inference endpoint information (only included if requested)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub endpoint: Option<super::inference_endpoints::InferenceEndpointResponse>,
}

impl From<DeploymentDBResponse> for DeployedModelResponse {
    fn from(db: DeploymentDBResponse) -> Self {
        Self {
            id: db.id,
            model_name: db.model_name,
            alias: db.alias,
            description: db.description,
            model_type: db.model_type,
            capabilities: db.capabilities,
            created_by: Some(db.created_by),
            hosted_on: db.hosted_on,
            created_at: db.created_at,
            updated_at: db.updated_at,
            requests_per_second: db.requests_per_second,
            burst_size: db.burst_size,
            capacity: db.capacity,
            batch_capacity: db.batch_capacity,
            groups: None,             // By default, relationships are not included
            metrics: None,            // By default, metrics are not included
            status: None,             // By default, probe status is not included
            pricing: None,            // By default, pricing is not included (opt-in via include)
            downstream_pricing: None, // By default, downstream pricing is not included
            endpoint: None,           // By default, endpoint is not included
        }
    }
}

impl DeployedModelResponse {
    /// Create a response with groups included
    pub fn with_groups(mut self, groups: Vec<GroupResponse>) -> Self {
        self.groups = Some(groups);
        self
    }

    /// Create a response with metrics included
    pub fn with_metrics(mut self, metrics: ModelMetrics) -> Self {
        self.metrics = Some(metrics);
        self
    }

    /// Create a response with probe status included
    pub fn with_status(mut self, status: ModelProbeStatus) -> Self {
        self.status = Some(status);
        self
    }

    /// Create a response with customer pricing included
    pub fn with_pricing(mut self, pricing: Option<TokenPricing>) -> Self {
        self.pricing = pricing;
        self
    }

    /// Create a response with downstream pricing included (admin only)
    pub fn with_downstream_pricing(mut self, downstream_pricing: Option<ProviderPricing>) -> Self {
        self.downstream_pricing = downstream_pricing;
        self
    }

    /// Mask rate limiting information (sets to None for users without permission)
    pub fn mask_rate_limiting(mut self) -> Self {
        self.requests_per_second = None;
        self.burst_size = None;
        self
    }

    /// Mask capacity information (sets to None for users without permission)
    pub fn mask_capacity(mut self) -> Self {
        self.capacity = None;
        self.batch_capacity = None;
        self
    }

    /// Mask created_by field (sets to None for users without system access)
    pub fn mask_created_by(mut self) -> Self {
        self.created_by = None;
        self
    }

    /// Create a response with endpoint information included
    pub fn with_endpoint(mut self, endpoint: super::inference_endpoints::InferenceEndpointResponse) -> Self {
        self.endpoint = Some(endpoint);
        self
    }
}
