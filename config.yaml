# Clay configuration
# This is the default configuration file.

# Server configuration
# To advertise publically, set to "0.0.0.0", or the specific network interface
# you've exposed.
host: "0.0.0.0"
port: 3001

# Database configuration
database:
  # Configure connectivity to an external postgres database here
  type: external
  url: postgres://postgres:password@localhost:5432/dwctl

  # Optional: Read replica URL for read-heavy operations (analytics, config sync)
  # replica_url: postgres://postgres:password@replica:5432/dwctl
  # Can also be set via environment variables (in order of precedence):
  #   DATABASE_REPLICA_URL=postgres://postgres:password@replica:5432/dwctl (recommended)
  #   DWCTL_DATABASE_REPLICA_URL=postgres://postgres:password@replica:5432/dwctl (single underscore)
  # Note: DWCTL_DATABASE__REPLICA_URL (double underscore) won't work due to config structure

  # Main database pool settings for primary (and replica if not specified separately)
  # pool:
  #   max_connections: 10
  #   min_connections: 0
  #   acquire_timeout_secs: 30
  #   idle_timeout_secs: 600
  #   max_lifetime_secs: 1800

  # Optional: Separate pool settings for replica connections
  # If not specified, replica uses the same settings as pool
  # replica_pool:
  #   max_connections: 5  # Typically fewer connections for replicas
  #   min_connections: 0
  #   acquire_timeout_secs: 30
  #   idle_timeout_secs: 600
  #   max_lifetime_secs: 1800

  # We also bundle a copy of postgres with dwctl. It's launched on a random
  # port, and data is stored in $HOME/.dwctl_data/ To have data persist
  # across restarts, set persistent=true.
  # We strongly recommend setting up external postgres in production!
  # type: embedded
  # persistent: true

  # Component databases: fusillade (batch processing) and outlet (request logging)
  # By default, these use separate schemas within the main database.
  # You can optionally configure them to use dedicated databases.

  # Fusillade - batch processing database
  # Default: uses "fusillade" schema in main database (inherits main's replica if configured)
  # fusillade:
  #   mode: schema
  #   name: fusillade
  #   pool:
  #     max_connections: 20
  #     min_connections: 2
  #   # Optional: separate replica pool settings (defaults to pool settings)
  #   # replica_pool:
  #   #   max_connections: 10
  #
  # Alternative: use a dedicated database for fusillade
  # fusillade:
  #   mode: dedicated
  #   url: postgres://postgres:password@localhost:5432/fusillade
  #   replica_url: postgres://postgres:password@replica:5432/fusillade
  #   pool:
  #     max_connections: 20
  #   # Optional: separate replica pool settings (defaults to pool settings)
  #   # replica_pool:
  #   #   max_connections: 10

  # Outlet - request logging database
  # Default: uses "outlet" schema in main database (inherits main's replica if configured)
  # outlet:
  #   mode: schema
  #   name: outlet
  #   pool:
  #     max_connections: 5
  #   # Optional: separate replica pool settings (defaults to pool settings)
  #   # replica_pool:
  #   #   max_connections: 3
  #
  # Alternative: use a dedicated database for outlet
  # outlet:
  #   mode: dedicated
  #   url: postgres://postgres:password@localhost:5432/outlet
  #   pool:
  #     max_connections: 5
  #   # Optional: separate replica pool settings (defaults to pool settings)
  #   # replica_pool:
  #   #   max_connections: 3

# Admin user email - will be created on first startup
admin_email: "test@doubleword.ai"
# TODO: Change this in production!
admin_password: "hunter2"

# Secret key for jwt signing.
# TODO: Change this in production
secret_key: insecure-change-in-production

# Payment configuration
# Dummy provider is enabled by default for testing
# For production, configure Stripe or another provider
payment:
  dummy:
    host_url: "http://localhost:5173"
    amount: 100.00

# Model sources - inference endpoints to connect to
# Uncomment and configure as needed
model_sources: []

# Example configurations:
# model_sources:
#   # OpenAI API
#   - name: "openai"
#     url: "https://api.openai.com"
#     api_key: "sk-..."  # Required for model sync
#
#   # Internal model server (no auth required)
#   - name: "internal"
#     url: "http://localhost:8080"

# Frontend metadata
metadata:
  title: "Doubleword Control Layer"
  # region: "UK South"
  # organization: "ACME Corp"
  docs_url: "https://docs.doubleword.ai/batches"
  docs_jsonl_url: "https://docs.doubleword.ai/batches/jsonl-files"

# Authentication configuration
auth:
  # Native username/password authentication
  native:
    enabled: true # Enable native login system
    allow_registration: true
    password:
      min_length: 8
      max_length: 64
    session:
      timeout: "24h"
      cookie_name: "dwctl_session"
      cookie_secure: true
      cookie_same_site: "strict"
    # Email configuration for password resets and notifications
    email:
      # Email transport - either 'file' (for development) or 'smtp' (for production)
      type: file
      path: "./emails" # Directory for file-based email (when type=file)
      # For SMTP (production), use:
      # type: smtp
      # host: "smtp.example.com"
      # port: 587
      # username: "noreply@example.com"
      # password: "your-smtp-password"
      # use_tls: true
      from_email: "noreply@example.com"
      from_name: "Control Layer"
      password_reset:
        token_expiry: "30m" # How long reset tokens are valid
        base_url: "http://localhost:3001" # Frontend URL for reset links

  # Proxy header authentication
  # Accepts user identity from HTTP headers set by an upstream authentication proxy
  # (e.g., oauth2-proxy, Vouch, Authentik, Auth0)
  #
  # Two modes:
  #   Single header: Send only header_name with user's email (must be unique)
  #   Dual header: Send both header_name (IdP identifier) and email_header_name (email)
  #                Allows multiple accounts per email from different identity providers
  proxy_header:
    enabled: true

    # header_name: User identifier or email
    # Single header mode: User's email (e.g., "user@example.com")
    # Dual header mode: Unique identifier from IdP (e.g., "github|user123", "google-oauth2|456")
    header_name: "x-doubleword-user"

    # email_header_name: User's email address (optional, enables dual header mode)
    # If provided: Enables federated identity with (email, external_user_id) uniqueness
    # If omitted: Uses header_name value as email (single header mode, email must be unique)
    email_header_name: "x-doubleword-email"

    # auto_create_users: Automatically create users on first login
    auto_create_users: true

  # Default roles assigned to newly created non-admin users
  # Applies to both native user registration and proxy header auto-creation
  # StandardUser role is always guaranteed to be present even if not specified
  # Available roles:
  #   - StandardUser: Required base role for all users (cannot be removed)
  #   - RequestViewer: Read-only access to request logs and analytics
  #   - BillingManager: Full access to credit system and user billing
  #   - BatchAPIUser: Can manage files and batches for batch processing
  # Example: To give all new users request viewing capabilities:
  #   default_user_roles: ["StandardUser", "RequestViewer"]
  default_user_roles: ["StandardUser", "BatchAPIUser"]

  # Security settings
  security:
    jwt_expiry: "1h"
    cors:
      allowed_origins:
        - "http://localhost:5173" # Development frontend (Vite)
        - "http://localhost:3001" # Local control layer
        - "https://localhost" # Local HTTPS
      allow_credentials: true
      max_age: 3600 # Cache preflight requests for 1 hour
      # exposed_headers: []  # Optional: Custom headers to expose to browser (e.g., ["X-Incomplete", "X-Last-Line"])

# Credits configuration
credits:
  # Initial credits given to standard users when they are created
  # Set to 0 to disable initial credits (users start with no credits)
  # Example: 100.00 gives users 100 credits on registration
  initial_credits_for_standard_users: 0

# Optional feature toggles
enable_metrics: true # Enable Prometheus metrics endpoint
enable_request_logging: true # Enable request/response logging to database
# Note: Environment variables can override top level setting, as long as they're supplied with the DWCTL_ prefix:
# DWCTL_PORT=8080
#
# The exception is DATABASE_URL:
# DATABASE_URL=postgres://prod@db:5432/my_database
#
# For arrays like model_sources, use environment-specific config files
# (e.g., config.production.yaml) rather than env vars.

# Batches API configuration
batches:
  # Enable batches API endpoints (/files, /batches)
  # When disabled, these endpoints will not be available (default: true)
  enabled: true

  # Allowed completion windows (SLAs) for batch processing
  # These define the maximum time from batch creation to completion
  # Default: ["24h"] - only 24 hour completion window is allowed
  allowed_completion_windows:
    - "24h"
    - "1h"
    # - "12h"  # Uncomment to allow 12 hour SLA
    # - "48h"  # Uncomment to allow 48 hour SLA

  # Files configuration for batch file uploads/downloads
  # NOTE: max_file_size has moved to limits.files.max_file_size
  files:
    upload_buffer_size: 100 # Buffer size for file upload streams (default: 100)
    download_buffer_size: 100 # Buffer size for file download streams (default: 100)
    # Database insertion batch size of request templates for file uploads
    # Higher values improve throughput for large files but use more memory
    # Reduce value if you encounter memory spike issues with many concurrent uploads
    batch_insert_size: 5000

# Resource limits configuration
# Controls file size, request count, and concurrency limits to prevent resource exhaustion
# limits:
#   files:
#     # Maximum file size in bytes (default: 100MB)
#     # Set to 0 for unlimited (not recommended for production)
#     max_file_size: 104857600
#     # Maximum number of requests (JSONL lines) per file
#     # Set to 0 for unlimited (default, not recommended for production)
#     max_requests_per_file: 50000
#     # Maximum concurrent file uploads allowed system-wide
#     # Set to 0 for unlimited (default, not recommended for production)
#     max_concurrent_uploads: 10
#     # Maximum uploads that can wait in queue for a slot
#     # When reached, new uploads receive HTTP 429 immediately
#     # Set to 0 for unlimited waiting queue (not recommended)
#     max_waiting_uploads: 20
#     # Maximum seconds to wait for an upload slot before returning HTTP 429
#     # Set to 0 to reject immediately when no slot is available
#     max_upload_wait_secs: 60

# Background services configuration
# Controls which background services run on this instance
background_services:
  # Onwards config sync - syncs database changes to the AI proxy routing layer
  # Disabling this will prevent the AI proxy from receiving config updates
  onwards_sync:
    enabled: true # Default: true (recommended)

  # Probe scheduler - periodically checks inference endpoint health
  # When leader_election is enabled, only runs on the elected leader
  probe_scheduler:
    enabled: true # Default: true

  # Batch processing daemon - processes batch requests asynchronously
  batch_daemon:
    # Controls when the batch processing daemon runs
    # - "always": Run on all instances (default)
    # - "leader": Only run on the elected leader instance
    # - "never": Never run the daemon
    enabled: always

    # Performance & Concurrency Settings
    claim_batch_size: 100 # Maximum number of requests to claim in each iteration
    default_model_concurrency: 10 # Default concurrent requests per model
    claim_interval_ms: 1000 # Milliseconds to sleep between claim iterations

    # Retry & Backoff Settings
    # All retry parameters are optional and default to fusillade's built-in defaults
    # min_retries: 3 - Minimum retries guaranteed regardless of other limits
    # max_retries: None - No cap on retries (only limited by deadline if set)
    # stop_before_deadline_ms: 900000 - Stop 15 minutes before batch deadline

    backoff_ms: 1000 # Initial backoff duration in milliseconds
    backoff_factor: 2 # Exponential backoff multiplier
    max_backoff_ms: 10000 # Maximum backoff duration in milliseconds

    # Timeout Settings
    timeout_ms: 600000 # Timeout per request attempt (10 minutes)
    claim_timeout_ms: 60000 # Max time in "claimed" state before auto-unclaim (1 minute)
    processing_timeout_ms: 600000 # Max time in "processing" state before auto-unclaim (10 minutes)

    # Observability
    status_log_interval_ms: 2000 # Interval for logging daemon status (set to null to disable)

    # SLA Daemon configuration
    # Route-at-claim-time model escalation
    # When a batch is within `escalation_threshold_seconds` of expiry at claim time,
    # the daemon routes requests to a different (typically faster) model.
    # Batch API keys automatically have access to escalation models in the routing cache.
    # model_escalations:
    #   gpt-4:  # Original model
    #     escalation_model: o1-preview  # Route to this model when near expiry
    #     escalation_threshold_seconds: 900  # Default: 900 (15 minutes before expiry)

  # Leader election - coordinates which instance runs leader-only services
  # When disabled, all instances run as leader (useful for single-instance deployments)
  leader_election:
    enabled: true # Default: true
